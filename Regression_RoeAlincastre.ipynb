{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Lab 1: Regression: Linear and Logistic"],"metadata":{"id":"EQCmniqYzNDT"}},{"cell_type":"markdown","source":["# **About this Lab:**\n","\n","Throughout this lab, there are Graded Activities where you will be tasked with implementing critical blocks of code and submitting them for grading. The detail marking guide can be found on Moodle. This lab is 1% of total course weight"],"metadata":{"id":"Jj_DWdHu4ewF"}},{"cell_type":"markdown","source":["## Part -1 Linear Regression\n","\n","Linear Regression is a statistical approach of modelling the realtionship between a scalar response (aka. dependent variable) and one or more explanatory variables (aka independent variables). Linear Regression is one of the most basic building block in Machine Learning algorithms.\n","\n","In most of the python libraries, linear regression model is available as a blackbox. However, it is imperative to understand what goes on under the hood in order to have better grasp of algorithm.\n","\n","The first part of this tutorial will focus on implementing vanilla linear regression from scratch using numpy and pandas."],"metadata":{"id":"vm-kc4xUzabg"}},{"cell_type":"markdown","source":["Import libraries"],"metadata":{"id":"BqZv3YklziH3"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn import datasets\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"CmDbdWNOzkKt","executionInfo":{"status":"ok","timestamp":1737233805005,"user_tz":420,"elapsed":10398,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### Getting Familiar with the Data\n","\n","Add characteristics of Data from: https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf"],"metadata":{"id":"cu6CUAyQzk6g"}},{"cell_type":"code","source":["def load_diabetes_data():\n","    diabetes = datasets.load_diabetes()\n","    X, Y = diabetes['data'], diabetes['target'].reshape(len(diabetes['target']), 1)\n","    return diabetes, X, Y\n","\n","def get_scale_object(data):\n","    scaler = MinMaxScaler()\n","    scaler.fit(data)\n","    return scaler\n","\n","def get_scaled_data(scaler, data):\n","    return scaler.transform(data)"],"metadata":{"id":"WeKP4BfmzoBS","executionInfo":{"status":"ok","timestamp":1737233818705,"user_tz":420,"elapsed":158,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["diabetes, X, Y = load_diabetes_data()\n","scaler = get_scale_object(X)"],"metadata":{"id":"BEL4vCrVzqnr","executionInfo":{"status":"ok","timestamp":1737233820270,"user_tz":420,"elapsed":155,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n","\n","m_train = X_train.shape[0]\n","m_test = X_test.shape[0]\n","n = X_train.shape[1]\n","\n","print (\"Number of training examples: m_train = \" + str(m_train))\n","print (\"Number of testing examples: m_test = \" + str(m_test))\n","print (\"Number of features for each instance: num_features = \" + str(n))\n","print(\"Features Names \", diabetes.feature_names)\n","print (\"train_set_x shape: \" + str(X_train.shape))\n","print (\"train_set_y shape: \" + str(Y_train.shape))\n","print (\"test_set_x shape: \" + str(X_test.shape))\n","print (\"test_set_y shape: \" + str(Y_test.shape))\n","# scaler = get_scale_object(raw_X)\n","# X = get_scaled_data(scaler, raw_X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZKfrLHc7zuOj","outputId":"18829b04-1f7b-47ef-b308-476bf818f6ea","executionInfo":{"status":"ok","timestamp":1737233821232,"user_tz":420,"elapsed":150,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training examples: m_train = 353\n","Number of testing examples: m_test = 89\n","Number of features for each instance: num_features = 10\n","Features Names  ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n","train_set_x shape: (353, 10)\n","train_set_y shape: (353, 1)\n","test_set_x shape: (89, 10)\n","test_set_y shape: (89, 1)\n"]}]},{"cell_type":"markdown","source":["### General Architecture of the Linear Regression"],"metadata":{"id":"TX1ma47czxm4"}},{"cell_type":"markdown","source":["### Building Parts:\n","1. Initialise Model Parameters\n","2. Loop:\n","\n","  a. Forward Propagation\n","\n","  b. Cost Function\n","\n","  c. Gradient Descent"],"metadata":{"id":"sSrf8HFlzyBV"}},{"cell_type":"markdown","source":["#### Initialise Model Parameters"],"metadata":{"id":"rd3CmZuhz56a"}},{"cell_type":"code","source":["def initialize_dimensions(X):\n","    m, n = X.shape\n","    return m, n\n","\n","def initialize_weight_vectors(dim):\n","    W = np.zeros(shape = dim)\n","    b = 0\n","    return W, b\n","\n","m_train, n = initialize_dimensions(X_train)\n","W, b = initialize_weight_vectors((1, n))\n","W.shape, b"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"leXOkB9Fz5g5","outputId":"a6127885-9372-4c58-ce15-91e4d79fc9f2","executionInfo":{"status":"ok","timestamp":1737233824274,"user_tz":420,"elapsed":147,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1, 10), 0)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["#### Forward Propagation and Cost Function\n"],"metadata":{"id":"4zEx1UApz_zE"}},{"cell_type":"code","source":["def compute_regression_cost(y, y_pred, m):\n","    cost = (1.0/(2*m)) * (np.sum(y_pred - y)**2)\n","    return cost\n","\n","def compute_regression_gradients(y_pred, y, X, m):\n","    dw = (1.0/m)* np.dot((y_pred - y).T, X)\n","    db = (1.0/m) * np.sum((y_pred - y))\n","    return dw, db\n","\n","def propagation(w, b, X, Y):\n","    # FORWARD PROPAGATION (FROM X TO COST)\n","\n","    z = (np.dot(X,w.T) + b)\n","    cost = compute_regression_cost(Y, z, X.shape[1])\n","\n","    #END YOUR CODE\n","\n","    # BACKWARD PROPAGATION (TO FIND GRAD)\n","\n","    dw, db = compute_regression_gradients(z, Y, X, X.shape[0])\n","\n","    cost = np.squeeze(cost)\n","\n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    #END YOUR CODE\n","    return grads, cost"],"metadata":{"id":"_jW8FW-G0CYF","executionInfo":{"status":"ok","timestamp":1737233826695,"user_tz":420,"elapsed":141,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Gradient Descent Optimization"],"metadata":{"id":"ZggyUNxL0G5-"}},{"cell_type":"code","source":["def train_linear_regression(X, Y, learning_rate = 0.2, n_epochs = 1000, print_cost = True):\n","    m, n = initialize_dimensions(X)\n","    print(f\"There are {n} features and {m} training examples\")\n","    w, b = initialize_weight_vectors((1, n))\n","    print(f\"W has a shape of {w.shape[0]} rows and {w.shape[1]} columns\")\n","    # print(f\"b has a shape of {b.shape[0]} rows and {b.shape[1]} columns\")\n","\n","    costs = []\n","    for iteration in range(n_epochs):\n","        grad, cost = propagation(w, b, X, Y)\n","\n","        dw = grad['dw']\n","        db = grad['db']\n","        # import pdb\n","        # pdb.set_trace()\n","        w = w - learning_rate * dw\n","        b = b - learning_rate * db\n","\n","        # Record the costs\n","        if iteration % 10 == 0:\n","            costs.append(cost)\n","\n","        # Print the cost every 100 training examples\n","        if print_cost and iteration % 100 == 0:\n","            y_pred = np.dot(X, w.T) + b\n","            accuracy = np.sqrt(np.mean((y_pred - Y)**2))\n","            display(\"Cost after iteration %i: %f | rmse after iteration %i: %f\" % (iteration, cost, iteration, accuracy))\n","\n","    params = {\"w\": w,\n","              \"b\": b}\n","\n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","\n","    return params, grads, costs"],"metadata":{"id":"KgwuiiF-0MLS","executionInfo":{"status":"ok","timestamp":1737233828652,"user_tz":420,"elapsed":153,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Prediction"],"metadata":{"id":"9mw90G5z0PGI"}},{"cell_type":"code","source":["def predict(w, b, X):\n","  y_predicted = (np.dot(X, params[\"w\"].T) + params[\"b\"])\n","  return y_predicted"],"metadata":{"id":"8ykfvj1H0Pvz","executionInfo":{"status":"ok","timestamp":1737233830961,"user_tz":420,"elapsed":147,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Model Evaluation\n","\n","R2 Score"],"metadata":{"id":"WrN66Z3Q0Te4"}},{"cell_type":"code","source":["def calculate_r2_score(y_true, y_pred):\n","\n","  ssr = (np.mean((y_pred - y_true)**2))\n","  sst = (np.mean((np.mean(y_true) - y_true)**2))\n","\n","  R2_score = 1.0 - (ssr/sst)\n","\n","  return R2_score"],"metadata":{"id":"8QWSkzIS0Wk6","executionInfo":{"status":"ok","timestamp":1737233832386,"user_tz":420,"elapsed":107,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["params, _, _ = train_linear_regression(X_train, Y_train, print_cost= True)\n","\n","y_predicted = (np.dot(X_test, params[\"w\"].T) + params[\"b\"])\n","\n","rmse = np.sqrt(np.mean((y_predicted - Y_test)**2))\n","print(calculate_r2_score(Y_test, y_predicted))\n","# display(\"Model R2 score is %f\"%R2_score)\n","\n","# from sklearn.metrics import r2_score\n","# r2_score(Y_test, y_predicted)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":249},"id":"BBCjiGT10Y-N","outputId":"240e684a-a151-4f7b-e9ea-0379628c27f2","executionInfo":{"status":"ok","timestamp":1737233834139,"user_tz":420,"elapsed":146,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 10 features and 353 training examples\n","W has a shape of 1 rows and 10 columns\n"]},{"output_type":"display_data","data":{"text/plain":["'Cost after iteration 0: 142711531.250000 | rmse after iteration 0: 143.257858'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'Cost after iteration 100: 0.424923 | rmse after iteration 100: 72.040547'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'Cost after iteration 200: 0.289734 | rmse after iteration 200: 68.547067'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'Cost after iteration 300: 0.197304 | rmse after iteration 300: 65.855888'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'Cost after iteration 400: 0.134167 | rmse after iteration 400: 63.770476'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'Cost after iteration 500: 0.091086 | rmse after iteration 500: 62.139237'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'Cost after iteration 600: 0.061726 | rmse after iteration 600: 60.847491'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'Cost after iteration 700: 0.041744 | rmse after iteration 700: 59.809771'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'Cost after iteration 800: 0.028166 | rmse after iteration 800: 58.963052'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["'Cost after iteration 900: 0.018955 | rmse after iteration 900: 58.261154'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0.40040972315107415\n"]}]},{"cell_type":"markdown","source":["# Logistic Regression"],"metadata":{"id":"Kl9hqX480zjz"}},{"cell_type":"markdown","source":["### Overview of the Problem Statement\n","\n","In 2020, there were 2.3 million women diagnosed with breast cancer and 685 000 deaths globally. With this, detecting breast cancer and determining whether its tumour is malignant or benign is a key challenge. In this tutorial, we will build a Logistic Regression classifier that learns to diagnose breast cancer as benign or malignant based on its tumour attributes. To train our model, we will leverage the copy of the Breast Cancer Wisconsin (Diagnostic) dataset provided by `sklearn`. Ten real-valued features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. These features describe the characteristics of each cell nucleus. They are listed as follows:\n","\n","1.   radius (mean of distances from the centre to points on the perimeter)\n","2.   texture (standard deviation of gray-scale values)\n","3.   perimeter\n","4.   area\n","5.   smoothness (local variation in radius lengths)\n","6.   compactness (perimeter^2 / area - 1.0)\n","7.   concavity (severity of concave portions of the contour)\n","7.   concave points (number of concave portions of the contour)\n","8.   symmetry\n","9.   fractal dimension (\"coastline approximation\" - 1)\n","\n","The mean, standard error, and “worst” or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in total *30 features*.\n","\n","More information about this dataset can be found on the following sites:\n","*   https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n","*   https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html"],"metadata":{"id":"b0TyLCpY02RQ"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import sklearn\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets"],"metadata":{"id":"XFlSjUpm05Rd","executionInfo":{"status":"ok","timestamp":1737233837383,"user_tz":420,"elapsed":113,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Getting Familiar with the Data and Notations\n","\n","*   n ⟶ number of features\n","*   m ⟶ number of training examples\n","*   X ⟶ input data of shape (m x n)\n","*   Y ⟶ target data of shape (m x 1)\n","*   x(i), y(i) ⟶ ith training example"],"metadata":{"id":"vmb2GTRY08BO"}},{"cell_type":"code","source":["def load_breast_cancer_dataset():\n","    '''\n","    This function loads the dataset.\n","    It also prints the description of the dataset.\n","\n","    Returns:\n","    X - Feature values of the samples.\n","    Y - Target variable. Takes value either 0(benign) or 1(malignant)\n","    '''\n","    data_dict = datasets.load_breast_cancer()\n","    X, Y = data_dict['data'], data_dict['target']\n","    Y = Y.reshape(len(Y), 1) # reshape the label vector so that it has a dimesion of m * k\n","\n","    print(\"Total number of instances in the dataset: \", X.shape[0])\n","    print(f\"There are {X.shape[1]} features as follows: \", data_dict.feature_names)\n","    return X, Y"],"metadata":{"id":"s29YNGoB0-iN","executionInfo":{"status":"ok","timestamp":1737233839348,"user_tz":420,"elapsed":152,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["X, Y = load_breast_cancer_dataset()\n","\n","# preprocess the features\n","scaler = MinMaxScaler()\n","X = scaler.fit_transform(X)\n","\n","# split into training and testing data\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hklVxIjw1AoU","outputId":"ac7d4646-6989-4eb7-c5f2-ec516583a3e7","executionInfo":{"status":"ok","timestamp":1737233841440,"user_tz":420,"elapsed":154,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of instances in the dataset:  569\n","There are 30 features as follows:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n"," 'mean smoothness' 'mean compactness' 'mean concavity'\n"," 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n"," 'radius error' 'texture error' 'perimeter error' 'area error'\n"," 'smoothness error' 'compactness error' 'concavity error'\n"," 'concave points error' 'symmetry error' 'fractal dimension error'\n"," 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n"," 'worst smoothness' 'worst compactness' 'worst concavity'\n"," 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"]}]},{"cell_type":"markdown","source":["<hr>\n","\n","> [Activity 1: 6 marks]: Set the values for:\n","- m_train → number of training instances\n","- m_test → number of test instances\n","- n → number of features\n","\n","> **Expected Output:**\n","\n",">|||\n","| ----------- | ----------- |\n","| m_train | 512 |\n","| m_test | 57 |\n","| n | 30 |\n","\n","<hr>"],"metadata":{"id":"DutwDPal1DQI"}},{"cell_type":"code","source":["\n","# # START YOUR CODE HERE\n","\n","m_train, n = X_train.shape\n","m_test = X_test.shape[0]\n","\n","\n","# # END YOUR CODE HERE\n","\n","print (\"Number of training examples: m_train = \" + str(m_train))\n","print (\"Number of testing examples: m_test = \" + str(m_test))\n","print (\"Number of features for each instance: num_features = \" + str(n))\n","\n","print (\"train_set_x shape: \" + str(X_train.shape))\n","print (\"train_set_y shape: \" + str(Y_train.shape))\n","print (\"test_set_x shape: \" + str(X_test.shape))\n","print (\"test_set_y shape: \" + str(Y_test.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xhma7T71I-7","outputId":"51a214f8-fe0a-4760-b349-571938e72d5b","executionInfo":{"status":"ok","timestamp":1737233849764,"user_tz":420,"elapsed":375,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training examples: m_train = 512\n","Number of testing examples: m_test = 57\n","Number of features for each instance: num_features = 30\n","train_set_x shape: (512, 30)\n","train_set_y shape: (512, 1)\n","test_set_x shape: (57, 30)\n","test_set_y shape: (57, 1)\n"]}]},{"cell_type":"markdown","source":["### General Architecture of the Logistic Regression\n","\n","The key steps in the implementation of logistic regression are:\n","\n","1. Initialise Model Parameters\n","\n","2. Loop:\n","\n","  a. Perform forward propagation\n","\n","  b. Compute cost and gradients\n","\n","  c. Update model parameters\n","\n","\n","We will build all these steps seperately and finally integrate them to define logistic regression model."],"metadata":{"id":"AHeeSTSo1Lix"}},{"cell_type":"markdown","source":["### 1. Initialise Model Parameters"],"metadata":{"id":"BIxocgqb1PjT"}},{"cell_type":"code","source":["def initialize_dimensions(X, Y):\n","    '''\n","    This function initializes the values of m, n and k\n","    m -> number of training instances\n","    n -> number of features\n","    k -> number of output labels (for binary classification, k= 1)\n","\n","    '''\n","    m, n = X.shape\n","    k = Y.shape[1]\n","    return m, n, k"],"metadata":{"id":"IAgdnzaD1P_u","executionInfo":{"status":"ok","timestamp":1737233858247,"user_tz":420,"elapsed":157,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def initialize_weights_with_zeros(dim):\n","    '''\n","    This function initializes the weights vector and bias vector\n","\n","    The genaral formula for Weights matrix and bias vector is -\n","    W -> number of output units x number of input features\n","    b -> 1 x number of output units\n","\n","    '''\n","    W = np.zeros(shape = (dim))\n","    b = 0\n","    return W, b"],"metadata":{"id":"JAN_8qzh1TNV","executionInfo":{"status":"ok","timestamp":1737233859885,"user_tz":420,"elapsed":138,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["Computing the sigmoid gradients:\n","* a) dW\n","* b) db\n","\n","Updating weights using the gradient descent equation"],"metadata":{"id":"J07tJ8ks1WRC"}},{"cell_type":"markdown","source":["### 2.a. Forward Propagation:\n","\n","The mathematical expression for forward propagation of logistic regression includes two steps:\n","\n","Step 1. Compute preactivations (Z) as:\n","\n","\\begin{equation}\n","Z = XW^T +b\n","\\end{equation}\n","\n","Step 2. Compute sigmoid activations ($\\hat{Y}$) for actual prediction:\n","\n","The sigmoid activation function limits the output to range between 0 and 1. The mathematical equation of a sigmoid activation function is given by:\n","\n","\\begin{equation}\n","\\hat{Y} = sigmoid(Z) = \\frac{1}{1 + e^{-(Z)}}\n","\\end{equation}\n","\n","<hr>\n","\n","> [Activity 2: 4 marks]: Use the equations above to implement `preactivation()` and then perform sigmoid transformation using `sigmoid()` for predicting $\\hat{Y}$.\n","\n","<hr>"],"metadata":{"id":"CjrxEoBQ1XHN"}},{"cell_type":"code","source":["def preactivation(X, w, b):\n","    '''\n","    This function computes the preactivation (Z) using weight and bias.\n","    '''\n","\n","    # # START YOUR CODE HERE\n","\n","    z = np.dot(X, w.T) + b\n","\n","    # # END YOUR CODE HERE\n","    return z\n","\n","def sigmoid(z):\n","    '''\n","    This function returns the sigmoid of vector (numpy array).\n","    '''\n","\n","    # # START YOUR CODE HERE\n","\n","    a = 1 / (1 + np.exp(-z))\n","\n","    # # END YOUR CODE HERE\n","    return a"],"metadata":{"id":"tshr8vmR1ejO","executionInfo":{"status":"ok","timestamp":1737233874682,"user_tz":420,"elapsed":158,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["Testing the implementation\n"],"metadata":{"id":"UCJu773v1h_w"}},{"cell_type":"code","source":["x1 =  np.array([[4.4, 0.9, 2.9], [3.2, 0.7, 2.6]]) # (m * n)\n","\n","w1 = np.array([[0.1, 0.2, 0.3]]) # (k * n)\n","b1 =  0 # (1 * k)\n","\n","print (\"preactivation for x1, w1, b1 = \" + str(preactivation(x1, w1, b1)))\n","print (\"sigmoid([[1.49], [1.24]]) = \" + str(sigmoid(np.array([[1.49], [1.24]]))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tzlfmli11kXL","outputId":"a42a438b-1146-454b-d17d-468c33e2c863","executionInfo":{"status":"ok","timestamp":1737233876841,"user_tz":420,"elapsed":158,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["preactivation for x1, w1, b1 = [[1.49]\n"," [1.24]]\n","sigmoid([[1.49], [1.24]]) = [[0.81607827]\n"," [0.77556401]]\n"]}]},{"cell_type":"markdown","source":["**Expected Output:**\n","\n","|||\n","|---|---|\n","preactivation for x1, w1, b1  | [[1.49] [1.24]]\n","sigmoid([[1.49], [1.24]]) | [[0.81607827] [0.77556401]]"],"metadata":{"id":"eVXOePi01mvC"}},{"cell_type":"markdown","source":["### 2.b. Compute Cost \\/Loss and Gradients.\n","\n","The mathematical expression of the logistic regression cost function is given by:\n","\n","\\begin{equation}\n","cost = - \\ \\frac{1}{m} \\sum_{i =1}^{m} y_i \\log(\\hat{Y}_i) + \\ (1-y_i) \\log(1-\\hat{Y}_i)\n","\\end{equation}\n","\n","It is important to note that, cost/error/loss is a scaler which we would like to minimize using gradient descent. The gradients of the cost function are derived using the chain rule.\n","\n","The final equations of the gradients are:\n","\n","\\begin{align}\n","\\\\\n","\\frac{\\partial L}{\\partial W}  = \\frac{1}{m}(\\hat{Y} - Y)^T X\n","\\\\\n","\\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i =1}^{m} (\\hat{Y} - Y)\n","\\end{align}\n","\n","<hr>\n","\n","> [Activity 3: 20 marks]: Use the equations above to compute cost using `compute_sigmoid_cost()`. Then implement `compute_sigmoid_gradients()` to compute its gradients *dw* and *db*.\n","\n","<hr>"],"metadata":{"id":"1z1JWXyy1qMz"}},{"cell_type":"code","source":["def compute_sigmoid_cost(Y, A, m):\n","    # # START YOUR CODE HERE\n","\n","    cost = -(1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n","\n","    # # END YOUR CODE HERE\n","    return cost\n","\n","def compute_sigmoid_gradients(X, A, Y, m):\n","    # # START YOUR CODE HERE\n","\n","    dw = (1 / m) * np.dot((A - Y).T, X)\n","    db = (1 / m) * np.sum(A - Y)\n","\n","    # # END YOUR CODE HERE\n","    return dw, db\n","\n","def propagation(w, b, X, Y):\n","    # # FORWARD PROPAGATION (FROM X TO COST)\n","    # START YOUR CODE HERE\n","\n","    prop = preactivation(X, w, b)\n","    activations = sigmoid(prop)\n","    cost = compute_sigmoid_cost(Y, activations, X.shape[0])\n","\n","    # END YOUR CODE HERE\n","    # # BACKWARD PROPAGATION (TO FIND GRAD)\n","    dw, db = compute_sigmoid_gradients(X, activations, Y, X.shape[0])\n","\n","    cost = np.squeeze(cost)\n","\n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","\n","    return grads, cost, activations"],"metadata":{"id":"3z_3Xmv21sCN","executionInfo":{"status":"ok","timestamp":1737233882496,"user_tz":420,"elapsed":158,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["\n","Testing the implementation\n"],"metadata":{"id":"JJLqASsl1vVW"}},{"cell_type":"code","source":["y1 = np.array([[1], [0]])\n","a1 =  np.array([[0.81607827], [0.77556401]]) # (m * 1)\n","m = 2\n","\n","w1 = np.array([[0.1, 0.2, 0.3]]) # (k * n)\n","b1 =  0 # (1 * k)\n","\n","\n","print (\"compute_sigmoid_cost = \" + str(compute_sigmoid_cost(y1, a1, m)))\n","\n","dw1, db1 = compute_sigmoid_gradients(x1, a1, y1, m)\n","print (\"compute_sigmoid_gradients dw1 = \"+str(dw1)+\" db1 = \"+str(db1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ld-lGq6h1xPP","outputId":"7230689f-6e2a-4011-b96c-e5aac2475bbb","executionInfo":{"status":"ok","timestamp":1737233901218,"user_tz":420,"elapsed":137,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["compute_sigmoid_cost = 0.8487048722248669\n","compute_sigmoid_gradients dw1 = [[0.83627461 0.18868262 0.7415467 ]] db1 = 0.29582114\n"]}]},{"cell_type":"markdown","source":["**Expected Output:**\n","\n","|||\n","|---|---|\n","compute_sigmoid_cost | [0.84870487]\n","|||\n","compute_sigmoid_gradients\n","dw1| [[0.83627461 0.18868262 0.7415467 ]]\n","db | 0.29582114\n"],"metadata":{"id":"y3uA_-Wk1zde"}},{"cell_type":"markdown","source":["### 2.c. Updating Model Parameters\n","\n","The weights can be updated using gradient descent equations as follows -\n","\\begin{align}\n","\\\\\n","W = W - (learning\\_rate * \\frac{\\partial L}{\\partial W})\n","\\\\\n","b = b - (learning\\_rate * \\frac{\\partial L}{\\partial b})\n","\\end{align}"],"metadata":{"id":"AKcWPNb-12LN"}},{"cell_type":"markdown","source":["### Assembling Logistic Regression Model\n","\n","<hr>\n","\n","> [Activity 4: 20 marks]: Now that we have implemented the helper functions above, it's time to use them and stitch our logistic regression model together.\n","\n","<hr>"],"metadata":{"id":"K0ZeEu9E14re"}},{"cell_type":"code","source":["def train_logistic_regression(X, Y, learning_rate = 0.02, n_epochs = 2000, print_cost = True):\n","\n","    m,n,k  = initialize_dimensions(X, Y)\n","\n","    # # initialise weights and biases with zeros\n","    # # START CODE HERE\n","    print(f\"There are {n} features and {m} training examples\")\n","    w, b = initialize_weights_with_zeros((1, n))\n","    print(f\"W has a shape of {w.shape[0]} rows and {w.shape[1]} columns\")\n","    # # END CODE HERE\n","\n","    costs = []\n","\n","    for iteration in range(n_epochs):\n","        # # perform one epoch to compute weight gradients and cost\n","        # # START CODE HERE\n","        grad, cost, activations = propagation(w, b, X, Y)\n","        # # END CODE HERE\n","\n","        dw = grad['dw']\n","        db = grad['db']\n","\n","        # # parameter update rule\n","        # # START CODE HERE\n","\n","        w = w - learning_rate * dw\n","        b = b - learning_rate * db\n","\n","        # # END CODE HERE\n","\n","        # Record the costs\n","        if iteration % 10 == 0:\n","            costs.append(cost)\n","\n","        # Write your code to Print the cost every 100 training examples\n","\n","        if iteration % 100 == 0:\n","            y_pred = (activations >= 0.5).astype(int)\n","            accuracy = np.mean(y_pred == Y) * 100\n","            print(f\"Cost after iteration {iteration}: {cost:.6f} | accuracy after iteration {iteration}: {accuracy}%\")\n","\n","    params = {\"w\": w, \"b\": b}\n","    grads = {\"dw\": dw, \"db\": db}\n","    return params, grads, costs"],"metadata":{"id":"Oa15d6ds17U9","executionInfo":{"status":"ok","timestamp":1737233910153,"user_tz":420,"elapsed":149,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["kl = train_logistic_regression(X_train, Y_train, print_cost= True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O2xshA4i19tv","outputId":"3515eafc-f049-4f75-fdaf-b9d9041db109","executionInfo":{"status":"ok","timestamp":1737233914613,"user_tz":420,"elapsed":826,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 30 features and 512 training examples\n","W has a shape of 1 rows and 30 columns\n","Cost after iteration 0: 0.693147 | accuracy after iteration 0: 63.0859375%\n","Cost after iteration 100: 0.640064 | accuracy after iteration 100: 75.9765625%\n","Cost after iteration 200: 0.596971 | accuracy after iteration 200: 84.1796875%\n","Cost after iteration 300: 0.560385 | accuracy after iteration 300: 86.71875%\n","Cost after iteration 400: 0.529048 | accuracy after iteration 400: 88.28125%\n","Cost after iteration 500: 0.501996 | accuracy after iteration 500: 89.0625%\n","Cost after iteration 600: 0.478453 | accuracy after iteration 600: 89.84375%\n","Cost after iteration 700: 0.457802 | accuracy after iteration 700: 90.625%\n","Cost after iteration 800: 0.439553 | accuracy after iteration 800: 91.015625%\n","Cost after iteration 900: 0.423314 | accuracy after iteration 900: 91.2109375%\n","Cost after iteration 1000: 0.408770 | accuracy after iteration 1000: 91.6015625%\n","Cost after iteration 1100: 0.395667 | accuracy after iteration 1100: 91.796875%\n","Cost after iteration 1200: 0.383798 | accuracy after iteration 1200: 91.796875%\n","Cost after iteration 1300: 0.372994 | accuracy after iteration 1300: 92.1875%\n","Cost after iteration 1400: 0.363113 | accuracy after iteration 1400: 92.1875%\n","Cost after iteration 1500: 0.354039 | accuracy after iteration 1500: 92.3828125%\n","Cost after iteration 1600: 0.345673 | accuracy after iteration 1600: 92.3828125%\n","Cost after iteration 1700: 0.337932 | accuracy after iteration 1700: 92.3828125%\n","Cost after iteration 1800: 0.330747 | accuracy after iteration 1800: 92.3828125%\n","Cost after iteration 1900: 0.324056 | accuracy after iteration 1900: 92.578125%\n"]}]},{"cell_type":"markdown","source":["### Prediction"],"metadata":{"id":"h9xTvZWR1_rY"}},{"cell_type":"code","source":["def predict(lr_model, X, threshold = 0.5):\n","\n","  '''\n","  This function uses the learned parameters of the\n","  model to classify the test data.\n","\n","  Sigmoid activation outputs the probability that the tumor for training\n","  instance x is malignant i.e. belongs to class 1.\n","\n","  For this demonstration, we will consider all probabilities ≥ 0.5 as\n","  belonging to class 1 and all probabilities < 0 = class 0.\n","  '''\n","\n","  w, b = lr_model[0][\"w\"],lr_model[0][\"b\"]\n","  y_predicted = sigmoid(np.dot(X, w.T) + b)\n","  Y_pred = np.array([1 if x>threshold else 0 for x in y_predicted])\n","  Y_pred = np.expand_dims(Y_pred, axis=1)\n","  return Y_pred"],"metadata":{"id":"JhLemIl32BPu","executionInfo":{"status":"ok","timestamp":1737233940511,"user_tz":420,"elapsed":133,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["Y_pred = predict(kl, X_test)"],"metadata":{"id":"x0Hv_JcW2DQ1","executionInfo":{"status":"ok","timestamp":1737233942803,"user_tz":420,"elapsed":133,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["### Evaluating Model Performance"],"metadata":{"id":"r0jBJTyV2E90"}},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n","\n","#START YOUR CODE HERE\n","\n","precision = precision_score(Y_test, Y_pred)\n","recall = recall_score(Y_test, Y_pred)\n","accuracy = accuracy_score(Y_test, Y_pred)\n","conf_matrix = confusion_matrix(Y_test, Y_pred)\n","\n","#END YOUR CODE HERE\n","\n","print(f\"Precision: \", precision,\" Accuracy: \", accuracy,\"Recall: \", recall)\n","print(\"Confusion Matrix\")\n","print(conf_matrix)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlmOIZHa2HDr","outputId":"3fc1076b-4e8c-44aa-9ac0-6dabae52a075","executionInfo":{"status":"ok","timestamp":1737233945438,"user_tz":420,"elapsed":134,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision:  0.918918918918919  Accuracy:  0.9473684210526315 Recall:  1.0\n","Confusion Matrix\n","[[20  3]\n"," [ 0 34]]\n"]}]}]}