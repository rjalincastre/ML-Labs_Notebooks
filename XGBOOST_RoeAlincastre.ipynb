{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Lab 8 XGBOOST**"],"metadata":{"id":"wcwAqqOWaL-X"}},{"cell_type":"markdown","source":["# **This lab is 3% of total Lab weight**"],"metadata":{"id":"G2r6-cJbaqam"}},{"cell_type":"markdown","source":["# Outline:\n","Powerful supervised machine learning algorithms.\n","\n","\n","1.   Recap of supervised learning.\n","2.   Discuss gradient boosting at a high level - Explain the need for boosting\n","3.   Introduce the idea of gradient boosting.\n","4.   Introduce XGBoost package - why is xgboost needed? Parallelization? Different APIs? How is it different from regular gradient boosting?\n","5.   Explain with a picture how XGBoost is used under the umbrella of ensemble methods. Provide a diagram to explain it.\n","\n","6. Show an example of training xgboost on a dataset? California Housing maybe -> Regression problem.\n","\n","7. Introduce the scikit-learn API for XGBoost\n","    1. Train an XGBoost classifier model maybe on MNIST?\n","    2. Obtain the accuracy.\n","\n","Activity:\n","\n","Go to the xgboost documentation and play around with the hyperparameters to improve model performance.\n","\n","Compare with logistic regression, can you improve on the accuracy by fiddling with the hyperparameters?\n","\n","Implement cross-validation with XGBoost\n","\n","\n","\n"],"metadata":{"id":"g9IdIDwhk8ty"}},{"cell_type":"markdown","source":["# XGBoost Tutorial"],"metadata":{"id":"aPr6aj2rjpJ4"}},{"cell_type":"markdown","source":["XGBoost stands for eXtreme Gradient Boosting.\n","In order to understand what XGBoost, we need to understand what gradient boosting is."],"metadata":{"id":"9uKhC6Qz2MmK"}},{"cell_type":"markdown","source":["## What is Boosting or Gradient Boosting?\n","The idea of boosting stems from the fact that 'weak learners' can be combined to form a strong model. Basically, we try to sequentially (or parallelly) add models for which we have weak predictions and optimize to improve on the weak predictions. **Gradient boosting**is simply the idea of optimizing the set of weak learners using a gradient descent-like procedure. The set of algorithms that use the gradient boosting technique are referred to as gradient boosting machines. Generally, boosting algorithms use trees as models, which are added sequentially."],"metadata":{"id":"tuPWSvYI-vLD"}},{"cell_type":"markdown","source":["## Why do we need boosting?\n","When solving a machine learning problem, it's possible that the chosen model does not perform well for some samples. One may refer to such a model as a weak learner because one does not have the best possible model. There are multiple ways to solve this problem, use a different model, change the optimization procedure, etc. A clever approach is to focus on improving the performance of the examples for which the model didn't perform well. This is the core idea of boosting."],"metadata":{"id":"Utwso1s1Gdos"}},{"cell_type":"markdown","source":["## Gradient Boosting - An explanation.\n","Gradient Boosting is used as a sequential/additive model training process where many weak models/learners are trained in sequence. In other words, they are trained iteratively. After many iterations or aggregation of the weak learners, we obtain the final ensemble model.\n","Given below is a visual explanation of how gradient boosting works."],"metadata":{"id":"bHhPAFgxgLlo"}},{"cell_type":"markdown","source":["![xgboost.drawio.png](data:image/png;base64,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)"],"metadata":{"id":"7Ol7pE8Ee0d-"}},{"cell_type":"markdown","source":["\n","\n","The model training starts with a simple model, and using its predictions and the ground truth, and we get an error. The errors from the first model then become the labels for the next model to be trained on. This process continues resulting in an ensemble model. We can repeat this procedure as many times as needed till we are satisfied with the ensemble model. This is also known as **additive training**. And each successive model is generally a tree-based model like a decision tree.\n","\n","\n","As a [mathematical formulation](https://xgboost.readthedocs.io/en/stable/tutorials/model.html#additive-training), this is what it looks like:\n","\n","\n","$\\hat{y}_i^{(0)} = 0$\n","\n","$\\hat{y}_i^{(1)} = f_1(x_i) = \\hat{y}_i^{(0)} + f_1(x_i)$\n","\n","$\\hat{y}_i^{(2)} = f_1(x_i) + f_2(x_i) = \\hat{y}_i^{(1)} + f_2(x_i)$\n","\n",".\n",".\n",".\n","\n","$\\hat{y}_i^{(t)} = \\sum_{k=1}^t f_k(x_i) = \\hat{y}_i^{(t-1)} + f_t(x_i)$\n","\n","\n","\n","We need an objective/cost function to train a machine learning model.\n","\n","$ojb^t = ∑_{i=1}^n l(y_i , \\hat(y)^{(t)} + ∑_{i=1}^t ω(f_i)$\n","\n","So, the only thing you need to change is your loss function to train a different model.\n","Fortunately, we don't have to implement the loss function by ourselves as we have a wonderful package to take care of it."],"metadata":{"id":"Ymn-lqLNIfud"}},{"cell_type":"markdown","source":["## Introduction to XGBoost\n","\n","Now that we have some ground work laid out, we are ready to talk about XGBoost.\n","XGBoost is one of the many supervised learning algorithms that has gained a lot of traction in recent times owing to its superior performance in many machine learning competitions.\n","\n","As said before, XGBoost stands for eXtreme Gradient Boosting. The name may be misleading as one might think of it to be as a new algorithm on top of gradient boosting. But simply, it's a more optimized version of the set of gradient boosting algorithms with the power of parallel computing. One important feature of XGBoost is that it supports regularization of gradient boosting algorithms thus preventing overfitting (which gradient boosting algorithms are susceptible to). XGBoost also supports parallelization of training on multiple CPUs simultaneously.\n","NOTE: XGBoost does not train the tree models independently on different cpus as gradient boosting in itself is a sequential procedure. But XGBoost paralellizes the creation of branches of the individual trees, which makes it faster."],"metadata":{"id":"5ltJ8FduJj8H"}},{"cell_type":"markdown","source":["## The XGBoost package\n","The XGBoost package is a versatile package providing support for multiple programming languages such as Python, R, Julia, Scala, C, C++, Ruby, Swift, and Java. We will be using the package for Python. XGBoost is also compatible with existing scientific computing python packages such as numpy and scipy. It also has support for pandas' dataframes. More information can be found [here](https://xgboost.readthedocs.io/en/stable/python/python_intro.html).\n","\n","XGBoost also provides multiple APIs for different use cases. For example, we will be using the scikit-learn XGBoost API. This means that we will be able to use the familiar functions `.fit` and `.predict` on our XGBoost models. To learn more about the scikit-learn XGBoost API, visit [this](https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn) link.\n","\n"],"metadata":{"id":"WFaEYcLTT8tv"}},{"cell_type":"markdown","source":["## Linear Regression with XGBoost"],"metadata":{"id":"LVevwabTV1ti"}},{"cell_type":"code","source":["# Import XGBoost package.\n","import xgboost as xgb\n","from xgboost import XGBRegressor\n","\n","# Let's also import the standard data science packages.\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Scikit-learn packages.\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import accuracy_score\n","\n","from sklearn.datasets import load_digits\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import KFold\n","from sklearn.datasets import fetch_california_housing\n","\n","from sklearn.linear_model import LogisticRegression"],"metadata":{"id":"LXM6vmf8JjbS","executionInfo":{"status":"ok","timestamp":1742687666019,"user_tz":360,"elapsed":6138,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["We'll train a regression model on the California housing dataset to predict house prices."],"metadata":{"id":"AnrZp43waR3M"}},{"cell_type":"code","source":["housing = fetch_california_housing()"],"metadata":{"id":"uOdn9kHMIfg9","executionInfo":{"status":"ok","timestamp":1742687667841,"user_tz":360,"elapsed":1821,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Let's see the description of the dataset."],"metadata":{"id":"WbDjf0mna5p2"}},{"cell_type":"code","source":["print(housing.DESCR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cNYZisr52s3l","outputId":"11f1ab3c-9f6b-4bb6-c89c-976a31381283","executionInfo":{"status":"ok","timestamp":1742687667861,"user_tz":360,"elapsed":10,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":[".. _california_housing_dataset:\n","\n","California Housing dataset\n","--------------------------\n","\n","**Data Set Characteristics:**\n","\n",":Number of Instances: 20640\n","\n",":Number of Attributes: 8 numeric, predictive attributes and the target\n","\n",":Attribute Information:\n","    - MedInc        median income in block group\n","    - HouseAge      median house age in block group\n","    - AveRooms      average number of rooms per household\n","    - AveBedrms     average number of bedrooms per household\n","    - Population    block group population\n","    - AveOccup      average number of household members\n","    - Latitude      block group latitude\n","    - Longitude     block group longitude\n","\n",":Missing Attribute Values: None\n","\n","This dataset was obtained from the StatLib repository.\n","https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n","\n","The target variable is the median house value for California districts,\n","expressed in hundreds of thousands of dollars ($100,000).\n","\n","This dataset was derived from the 1990 U.S. census, using one row per census\n","block group. A block group is the smallest geographical unit for which the U.S.\n","Census Bureau publishes sample data (a block group typically has a population\n","of 600 to 3,000 people).\n","\n","A household is a group of people residing within a home. Since the average\n","number of rooms and bedrooms in this dataset are provided per household, these\n","columns may take surprisingly large values for block groups with few households\n","and many empty houses, such as vacation resorts.\n","\n","It can be downloaded/loaded using the\n",":func:`sklearn.datasets.fetch_california_housing` function.\n","\n",".. rubric:: References\n","\n","- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n","  Statistics and Probability Letters, 33 (1997) 291-297\n","\n"]}]},{"cell_type":"markdown","source":["Let's print the dataset and see what's in it."],"metadata":{"id":"_WHSr94LbFgs"}},{"cell_type":"code","source":["housing"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l90zl5Cn1-so","outputId":"3bbcb8a9-2e21-4165-dde1-96ee8d548084","executionInfo":{"status":"ok","timestamp":1742687667864,"user_tz":360,"elapsed":2,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n","           37.88      , -122.23      ],\n","        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n","           37.86      , -122.22      ],\n","        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n","           37.85      , -122.24      ],\n","        ...,\n","        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n","           39.43      , -121.22      ],\n","        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n","           39.43      , -121.32      ],\n","        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n","           39.37      , -121.24      ]]),\n"," 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894]),\n"," 'frame': None,\n"," 'target_names': ['MedHouseVal'],\n"," 'feature_names': ['MedInc',\n","  'HouseAge',\n","  'AveRooms',\n","  'AveBedrms',\n","  'Population',\n","  'AveOccup',\n","  'Latitude',\n","  'Longitude'],\n"," 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 20640\\n\\n:Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n:Attribute Information:\\n    - MedInc        median income in block group\\n    - HouseAge      median house age in block group\\n    - AveRooms      average number of rooms per household\\n    - AveBedrms     average number of bedrooms per household\\n    - Population    block group population\\n    - AveOccup      average number of household members\\n    - Latitude      block group latitude\\n    - Longitude     block group longitude\\n\\n:Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nA household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surprisingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. rubric:: References\\n\\n- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n  Statistics and Probability Letters, 33 (1997) 291-297\\n'}"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"4rFZKoPdjlOm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d08b90a-4a8b-4d69-ec76-36a20e541670","executionInfo":{"status":"ok","timestamp":1742687667906,"user_tz":360,"elapsed":42,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n"]}],"source":["# These are the feature names.\n","print(housing.feature_names)"]},{"cell_type":"markdown","source":["Let's obtain the features and target values."],"metadata":{"id":"blfZ4_Ezb7RS"}},{"cell_type":"code","source":["X = housing.data\n","y = housing.target\n","\n","# These are numpy arrays so we can print out the shape of each variable.\n","print(\"Shape of input features: \", X.shape)\n","print(\"Shape of target variable: \", y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xBv0ejMPb4_N","outputId":"f81af78c-b1c3-4bc2-f7b2-f299678e436d","executionInfo":{"status":"ok","timestamp":1742687667913,"user_tz":360,"elapsed":8,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of input features:  (20640, 8)\n","Shape of target variable:  (20640,)\n"]}]},{"cell_type":"markdown","source":["So we have 20640 samples in total."],"metadata":{"id":"5zF1MJkScVxo"}},{"cell_type":"markdown","source":["Divide the dataset into training and test set."],"metadata":{"id":"ahvStCpycZss"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Use 20% of the dataset as test set."],"metadata":{"id":"7aPgObc8cHMC","executionInfo":{"status":"ok","timestamp":1742687667914,"user_tz":360,"elapsed":1,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["XGBoost supports both Regression and Classification models. Since our target variable is continuous, we will use a regression model."],"metadata":{"id":"DP53dMPkbSIu"}},{"cell_type":"markdown","source":["NOTE: The power of XGBoost comes from parallelizing computations.\n","By default, the number of parallel threads to run the code is set of 1. But we can change that by setting the `n_jobs` to the respective number of threads. A good thumb rule is to set the number of threads to number of cpu cores that are on the machine running the code.\n","\n","NOTE: Google Colab provides 2 CPUs in the default plan. Generally, the speedup is noticeable when XGBoost is trained on massive datasets with a lot of features, but for the California housing dataset, it may not be noticeable."],"metadata":{"id":"AuW_a9AQ5a9c"}},{"cell_type":"code","source":["# Now let's train an XGBoost regression model.\n","model = XGBRegressor(random_state=42, n_jobs=2)  # Initialize the XGBoost regression model. Fixing the random_state for reproducibility.\n","model.fit(X_train, y_train)  # Fit the model.\n","preds = model.predict(X_test)"],"metadata":{"id":"WMp3s8S2dD2k","executionInfo":{"status":"ok","timestamp":1742687672015,"user_tz":360,"elapsed":4101,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Yes! It's that simple!\n","Now let's evaluate the predictions. We can use mean squared error for it."],"metadata":{"id":"Zz2Hw9CwdzRr"}},{"cell_type":"code","source":["print(\"Mean squared error is: \", mean_squared_error(y_test, preds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R1x12Kknd62V","outputId":"2529fd9c-0a53-4687-85e4-61b636146724","executionInfo":{"status":"ok","timestamp":1742687672059,"user_tz":360,"elapsed":43,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean squared error is:  0.21611504491209652\n"]}]},{"cell_type":"markdown","source":["We can also calculate the $R^2$ value which gives us a measure of the proportion of the variation in the dependent variable that is predictable from the independent features."],"metadata":{"id":"6LIdqpHSek_L"}},{"cell_type":"code","source":["from sklearn.metrics import r2_score\n","r2_score(y_test, preds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pm-Yb91peRdM","outputId":"de863705-b470-4ccf-9b43-fe2ba435e64c","executionInfo":{"status":"ok","timestamp":1742687672111,"user_tz":360,"elapsed":45,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8417107619447489"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["That's a high $R^2$ score, which means we have a good model."],"metadata":{"id":"1q0PX_IAfK5w"}},{"cell_type":"markdown","source":["#Train XGBoost Classifier Model."],"metadata":{"id":"s5di2BxVgH_U"}},{"cell_type":"markdown","source":["Train a XGBoost Classifier to predict handwritten digits. You may use the MNIST dataset for this challenge. What performance do you get when using the default parameters? Refer to [this](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier) link on how to use XGBoost Classifier."],"metadata":{"id":"zE6c0iZ8g0Ik"}},{"cell_type":"markdown","source":["#Activity 1- Load the MNIST dataset from scikit-learn.\n","\n","Remember to import the scikit-learn datasets package."],"metadata":{"id":"Z1jDJGBVlkr-"}},{"cell_type":"code","source":["### START YOUR CODE HERE.\n","\n","# Import the datasets package from scikit-learn\n","from sklearn.datasets import load_digits\n","\n","# Load the MNIST dataset\n","digits = load_digits()\n","\n","### END OF YOUR CODE HERE"],"metadata":{"id":"4XPoAHHAlpqr","executionInfo":{"status":"ok","timestamp":1742687672218,"user_tz":360,"elapsed":106,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["#Activity 2- Now get the features and the target variables.\n","\n","Remember to reshape the feature into a 2-d matrix. Refer to [this](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html) link for instructions on how to correctly load the mnist dataset."],"metadata":{"id":"v9y6RWJymE0b"}},{"cell_type":"code","source":["### START YOUR CODE HERE.\n","# Load the features and target variables.\n","X = digits.data      # Already a 2D array: (n_samples, n_features)\n","y = digits.target    # Labels for each image\n","\n","### END OF YOUR CODE HERE"],"metadata":{"id":"PQtJ23hYmD5L","executionInfo":{"status":"ok","timestamp":1742687672219,"user_tz":360,"elapsed":0,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["#Activity 3- Now obtain the train and test sets from the features $X$, and the target $y$.\n","\n","\n"],"metadata":{"id":"JGvB0JVwm7kr"}},{"cell_type":"code","source":["### START YOUR CODE HERE.\n","# Train test split.\n","# Split into 80% training and 20% testing\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","### END OF YOUR CODE HERE"],"metadata":{"id":"D-QMURVjQ95K","executionInfo":{"status":"ok","timestamp":1742687672225,"user_tz":360,"elapsed":5,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["#Activity 4- Then initialize an XGBoost Classifier and train the model."],"metadata":{"id":"hIV-O6chRAmQ"}},{"cell_type":"code","source":["### START YOUR CODE HERE.\n","\n","from xgboost import XGBClassifier\n","\n","# Initialize the XGBoost Classifier\n","model = XGBClassifier(eval_metric=\"mlogloss\", random_state=42)\n","\n","# Fit the model\n","model.fit(X_train, y_train)\n","\n","# Predict on the test set\n","preds = model.predict(X_test)\n","\n","### END OF YOUR CODE HERE\n","\n","print(\"XGBoost Classifier accuracy with default hyperparameters is: \",accuracy_score(y_test, preds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3YxXpZhcm24I","outputId":"f50c6c2a-42f6-4f8f-bdf9-46bc088f41c8","executionInfo":{"status":"ok","timestamp":1742687676180,"user_tz":360,"elapsed":3955,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["XGBoost Classifier accuracy with default hyperparameters is:  0.9694444444444444\n"]}]},{"cell_type":"markdown","source":["#Implement K-fold cross-validation"],"metadata":{"id":"9NOsv_X5fb1k"}},{"cell_type":"markdown","source":["#Activity 5- Implement [k-fold cross-validation]\n","(https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) with XGBoost.  For each fold, get a different train and test set. Print the average accuracy over the different cross-validation folds. You may start with `k=5` and modify it as you wish."],"metadata":{"id":"wn5oB-nfhHDg"}},{"cell_type":"code","source":["### START YOUR CODE HERE - PART A\n","\n","# define parameters here\n","\n","# Do 5-fold cross-validation.\n","# Set number of folds\n","splits = 5\n","\n","# define a k spliter based on k-fold object of sckikit learn\n","# Create a KFold cross-validator with 5 splits and shuffling enabled\n","kf = KFold(n_splits=splits, shuffle=True, random_state=42)\n","\n","# List to store accuracy for each fold\n","fold_accuracies = []\n","\n","### END OF YOUR CODE HERE - PART A\n","\n","for train_idx, test_idx in kf.split(X):\n","### START YOUR CODE HERE - PART B\n","\n","    # Split the data\n","    X_train_fold, X_test_fold = X[train_idx], X[test_idx]\n","    y_train_fold, y_test_fold = y[train_idx], y[test_idx]\n","\n","    # Initialize and train model\n","    model = XGBClassifier(eval_metric='mlogloss', random_state=42)\n","    model.fit(X_train_fold, y_train_fold)\n","\n","    # Predict and evaluate\n","    preds_fold = model.predict(X_test_fold)\n","    accuracy = accuracy_score(y_test_fold, preds_fold)\n","\n","    # Append accuracy to the list\n","    fold_accuracies.append(accuracy)\n","\n","### END OF YOUR CODE HERE - PART B\n","\n","print(f\"All accuracies are: {fold_accuracies}\")\n","print(f\"Mean XGBoost classifier over k={splits} folds is : {np.mean(fold_accuracies)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"983M3loXp0Ku","outputId":"0d54be70-334b-41c7-d942-00ff55c5c49d","executionInfo":{"status":"ok","timestamp":1742687695798,"user_tz":360,"elapsed":19616,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["All accuracies are: [0.9694444444444444, 0.9722222222222222, 0.9442896935933147, 0.9777158774373259, 0.958217270194986]\n","Mean XGBoost classifier over k=5 folds is : 0.9643779015784586\n"]}]},{"cell_type":"markdown","source":["#Compare performance with Logistic Regression.\n","Compare the performance of XGBoost with Logistic Regression for classifying MNIST digits. You can reuse the code from the previous cell that uses cross-validation and then compare the performance over 5 folds.\n","\n","NOTE: Your implementation may take a while to finish. To avoid the converge warning, set the `max_iter` hyperparameter of the LogisticRegression model to 100000."],"metadata":{"id":"HLtZ_L4HudxD"}},{"cell_type":"markdown","source":["#Activity 6- Implement Logistic Regression"],"metadata":{"id":"PZRjk-0N0__v"}},{"cell_type":"code","source":["### START YOUR CODE HERE\n","\n","# Implement LogisticRegression.\n","\n","# Reuse k-fold splitter\n","fold_accuracies = []\n","\n","for train_idx, test_idx in kf.split(X):\n","    # Split data\n","    X_train_fold, X_test_fold = X[train_idx], X[test_idx]\n","    y_train_fold, y_test_fold = y[train_idx], y[test_idx]\n","\n","    # Initialize and train Logistic Regression model\n","    log_reg = LogisticRegression(max_iter=100000)\n","    log_reg.fit(X_train_fold, y_train_fold)\n","\n","    # Predict and evaluate\n","    preds_fold = log_reg.predict(X_test_fold)\n","    accuracy = accuracy_score(y_test_fold, preds_fold)\n","\n","    # Store the fold accuracy\n","    fold_accuracies.append(accuracy)\n","\n","### END OF YOUR CODE HERE\n","\n","print(f\"All accuracies are: {fold_accuracies}\")\n","print(f\"Mean LogReg classifier over k={splits} folds is : {np.mean(fold_accuracies)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAYpY0gls_J6","outputId":"bfb35925-328e-4339-8f0b-aa0d5a9397ca","executionInfo":{"status":"ok","timestamp":1742687697028,"user_tz":360,"elapsed":1232,"user":{"displayName":"Roe Alincastre","userId":"16501113537082474917"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["All accuracies are: [0.975, 0.9611111111111111, 0.9610027855153204, 0.9610027855153204, 0.958217270194986]\n","Mean LogReg classifier over k=5 folds is : 0.9632667904673475\n"]}]},{"cell_type":"markdown","source":["And that's how XGBoost works! You now understand the core fundamentals of the XGBoost framework and motivation behind it, and how to implement the algorithm to solve machine learning problems."],"metadata":{"id":"A7E8mwIVHcoB"}},{"cell_type":"markdown","source":["While both models performed well, based on the 5-fold cross-validation results, XGBoost achieved a slightly higher average accuracy compared to Logistic Regression. Therefore, XGBoost is slightly the better-performing model for classifying handwritten digits in this task."],"metadata":{"id":"M8lEIYhkTXRj"}}]}